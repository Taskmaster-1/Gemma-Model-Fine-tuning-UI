{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma Model Fine-tuning with TPU/GPU\n",
    "\n",
    "This notebook demonstrates how to fine-tune the Gemma model on custom datasets using either TPU or GPU accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "!pip install -q transformers datasets accelerate jax flax optax huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For TPU support\n",
    "import jax\n",
    "import flax.linen as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# Option 1: Load from local file\n",
    "dataset_path = \"../data/sample_dataset.csv\"  # Replace with your dataset path\n",
    "\n",
    "# Option 2: Load from Hugging Face Datasets\n",
    "# dataset = load_dataset(\"your_dataset_name\")\n",
    "\n",
    "# For local CSV file\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\": dataset_path})\n",
    "    print(f\"Dataset loaded successfully: {len(dataset['train'])} examples\")\n",
    "    # Preview dataset\n",
    "    print(dataset['train'][:3])\n",
    "else:\n",
    "    # Demo dataset for testing\n",
    "    print(\"Using demo dataset for testing...\")\n",
    "    data = {\n",
    "        \"text\": [\n",
    "            \"Gemma is a large language model developed by Google.\",\n",
    "            \"Fine-tuning allows adaptation of models to specific tasks.\",\n",
    "            \"This is a sample dataset for demonstration purposes.\"\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"demo_dataset.csv\", index=False)\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\": \"demo_dataset.csv\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your environment variable with your API key\n",
    "# os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"your_hf_token_here\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"google/gemma-2b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with device mapping for optimal hardware utilization\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # Adjust as needed\n",
    "    )\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset: {len(tokenized_dataset['train'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directory\n",
    "output_dir = \"./gemma-fine-tuned\"\n",
    "\n",
    "# Check available hardware\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Try to detect TPUs\n",
    "try:\n",
    "    tpu_devices = jax.devices('tpu')\n",
    "    print(f\"TPU devices available: {len(tpu_devices)}\")\n",
    "    using_tpu = True\n",
    "except:\n",
    "    print(\"No TPU devices detected\")\n",
    "    using_tpu = False\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,  # Adjust based on available memory\n",
    "    gradient_accumulation_steps=2,  # Increase for larger effective batch size\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True if device == \"cuda\" else False,  # Use mixed precision on GPU\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for testing\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Set pad token if needed\n",
    "if fine_tuned_tokenizer.pad_token is None:\n",
    "    fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token\n",
    "\n",
    "# Test generation function\n",
    "def generate_text(prompt, max_length=100, temperature=0.7, top_p=0.9):\n",
    "    inputs = fine_tuned_tokenizer(prompt, return_tensors=\"pt\").to(fine_tuned_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=fine_tuned_tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return fine_tuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with a sample prompt\n",
    "test_prompt = \"What is fine-tuning in machine learning?\"\n",
    "generated_text = generate_text(test_prompt)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Push to Hugging Face Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and complete to push your model to Hugging Face Hub\n",
    "\"\"\"\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login to Hugging Face\n",
    "notebook_login()\n",
    "\n",
    "# Push model to hub\n",
    "fine_tuned_model.push_to_hub(\"your-username/gemma-fine-tuned\")\n",
    "fine_tuned_tokenizer.push_to_hub(\"your-username/gemma-fine-tuned\")\n",
    "print(\"Model pushed to HuggingFace Hub: your-username/gemma-fine-tuned\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
